{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Setup and Dependencies\n",
    "\n",
    "This cell sets up the required environment for a language classification system:\n",
    "\n",
    "- Imports essential libraries:\n",
    "  - NLP: NLTK for text processing\n",
    "  - Data processing: NumPy, Pandas\n",
    "  - Machine Learning: Scikit-learn components\n",
    "  - Utilities: tqdm, warnings\n",
    "\n",
    "The cell also:\n",
    "- Downloads required NLTK resources: udhr, punkt, stopwords\n",
    "- Verifies NLTK functionality with a test sentence\n",
    "- Suppresses warnings for cleaner output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import udhr, gutenberg, brown, reuters\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords \n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import string\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download required NLTK data\n",
    "print(\"Downloading required NLTK resources...\")\n",
    "resources = ['udhr', 'punkt', 'stopwords']\n",
    "for resource in resources:\n",
    "    nltk.download(resource)\n",
    "\n",
    "print(\"\\nVerifying NLTK data...\")\n",
    "test_text = \"This is a test sentence.\"\n",
    "tokens = word_tokenize(test_text)\n",
    "print(\"NLTK resources verified successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced Language Feature Extractor\n",
    "\n",
    "Custom scikit-learn transformer that extracts linguistic features from text:\n",
    "\n",
    "## Key Features\n",
    "- TF-IDF vectorization with n-gram support\n",
    "- Text length metrics\n",
    "- Character-level statistics:\n",
    "  - Punctuation ratios\n",
    "  - Case ratios (upper/lowercase)\n",
    "  - Digit ratios\n",
    "  - Whitespace analysis\n",
    "  - Special character distribution\n",
    "\n",
    "## Methods\n",
    "- `get_advanced_features()`: Extracts detailed linguistic features\n",
    "- `fit()`: Trains the feature extractor\n",
    "- `transform()`: Converts texts into feature matrices\n",
    "\n",
    "Designed for integration with scikit-learn pipelines and comprehensive text analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedLanguageFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Enhanced feature extractor with linguistic features\"\"\"\n",
    "    \n",
    "    def __init__(self, n_gram_range=(1, 3), max_features=1000):\n",
    "        self.n_gram_range = n_gram_range\n",
    "        self.max_features = max_features\n",
    "        self.stopwords = set(stopwords.words('english'))\n",
    "        self.feature_names_ = None\n",
    "        self.tfidf = TfidfVectorizer(\n",
    "            max_features=max_features,\n",
    "            ngram_range=n_gram_range,\n",
    "            stop_words='english'\n",
    "        )\n",
    "        \n",
    "    def get_advanced_features(self, text):\n",
    "        \"\"\"Extract advanced linguistic features\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        # Text length features\n",
    "        features['total_length'] = len(text)\n",
    "        features['avg_word_length'] = np.mean([len(w) for w in text.split()])\n",
    "        \n",
    "        # Punctuation features\n",
    "        for punct in string.punctuation:\n",
    "            features[f'punct_ratio_{punct}'] = text.count(punct) / len(text)\n",
    "            \n",
    "        # Case features\n",
    "        features['uppercase_ratio'] = sum(1 for c in text if c.isupper()) / len(text)\n",
    "        features['lowercase_ratio'] = sum(1 for c in text if c.islower()) / len(text)\n",
    "        \n",
    "        # Digit features\n",
    "        features['digit_ratio'] = sum(1 for c in text if c.isdigit()) / len(text)\n",
    "        \n",
    "        # Whitespace features\n",
    "        features['space_ratio'] = text.count(' ') / len(text)\n",
    "        features['newline_ratio'] = text.count('\\n') / len(text)\n",
    "        \n",
    "        # Language-specific features\n",
    "        features['english_char_ratio'] = sum(1 for c in text if c in string.ascii_letters) / len(text)\n",
    "        features['special_char_ratio'] = sum(1 for c in text if not c.isalnum() and c not in string.whitespace) / len(text)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit the feature extractor\"\"\"\n",
    "        # Fit TF-IDF\n",
    "        self.tfidf.fit(X)\n",
    "        \n",
    "        # Get sample features to establish feature names\n",
    "        sample_features = self.get_advanced_features(X[0])\n",
    "        self.feature_names_ = list(sample_features.keys()) + self.tfidf.get_feature_names_out().tolist()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform texts into feature matrix\"\"\"\n",
    "        # Get TF-IDF features\n",
    "        tfidf_features = self.tfidf.transform(X).toarray()\n",
    "        \n",
    "        # Get advanced features\n",
    "        feature_matrix = []\n",
    "        for text in tqdm(X, desc=\"Extracting features\"):\n",
    "            advanced_features = self.get_advanced_features(text)\n",
    "            feature_vector = list(advanced_features.values())\n",
    "            feature_matrix.append(feature_vector)\n",
    "            \n",
    "        # Combine features\n",
    "        return np.hstack([np.array(feature_matrix), tfidf_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilingual Text Collector\n",
    "\n",
    "Text collection system that gathers and processes multilingual data from various sources:\n",
    "\n",
    "## Features\n",
    "- Collects English texts from:\n",
    "  - Project Gutenberg corpus\n",
    "  - Brown corpus\n",
    "- Gathers non-English texts from:\n",
    "  - Universal Declaration of Human Rights (UDHR) corpus\n",
    "\n",
    "## Key Functions\n",
    "- `collect_english_texts()`: Extracts English text samples\n",
    "- `collect_non_english_texts()`: Gathers non-English samples\n",
    "- `chunk_text()`: Splits texts into manageable chunks\n",
    "- `collect_dataset()`: Creates balanced dataset with specified sample limits\n",
    "\n",
    "Includes progress tracking and error handling for robust data collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilingualTextCollector:\n",
    "    \"\"\"Collect and preprocess multilingual text data\"\"\"\n",
    "    \n",
    "    def __init__(self, min_text_length=1000):\n",
    "        self.min_text_length = min_text_length\n",
    "    \n",
    "    def collect_english_texts(self):\n",
    "        \"\"\"Collect English texts from multiple sources\"\"\"\n",
    "        english_texts = []\n",
    "        \n",
    "        print(\"\\nCollecting English Texts:\")\n",
    "        print(\"-----------------------\")\n",
    "        \n",
    "        # Collect from Gutenberg\n",
    "        print(\"\\nSamples from Gutenberg:\")\n",
    "        for fileid in gutenberg.fileids():\n",
    "            try:\n",
    "                text = ' '.join(gutenberg.words(fileid))\n",
    "                chunks = self.chunk_text(text)\n",
    "                if chunks:\n",
    "                    english_texts.extend(chunks)\n",
    "                    print(f\"\\nFile: {fileid}\")\n",
    "                    print(\"Sample text:\")\n",
    "                    print(chunks[0][:200] + \"...\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing Gutenberg file {fileid}: {str(e)}\")\n",
    "        \n",
    "        # Collect from Brown corpus\n",
    "        print(\"\\nSamples from Brown corpus:\")\n",
    "        for fileid in brown.fileids():\n",
    "            try:\n",
    "                text = ' '.join(brown.words(fileid))\n",
    "                chunks = self.chunk_text(text)\n",
    "                if chunks:\n",
    "                    english_texts.extend(chunks)\n",
    "                    print(f\"\\nFile: {fileid}\")\n",
    "                    print(\"Sample text:\")\n",
    "                    print(chunks[0][:200] + \"...\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing Brown file {fileid}: {str(e)}\")\n",
    "        \n",
    "        return english_texts\n",
    "    \n",
    "    def collect_non_english_texts(self):\n",
    "        \"\"\"Collect non-English texts from UDHR\"\"\"\n",
    "        non_english_texts = []\n",
    "        print(\"\\nCollecting Non-English Texts:\")\n",
    "        print(\"---------------------------\")\n",
    "        \n",
    "        available_languages = [fid for fid in udhr.fileids() \n",
    "                             if fid != 'English-Latin1' and 'Latin1' in fid]\n",
    "        \n",
    "        for lang in tqdm(available_languages, desc=\"Collecting non-English texts\"):\n",
    "            try:\n",
    "                text = ' '.join(udhr.words(lang))\n",
    "                chunks = self.chunk_text(text)\n",
    "                if chunks:\n",
    "                    non_english_texts.extend(chunks)\n",
    "                    print(f\"\\nLanguage: {lang}\")\n",
    "                    print(\"Sample text:\")\n",
    "                    print(chunks[0][:200] + \"...\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing language {lang}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        return non_english_texts\n",
    "    \n",
    "    def chunk_text(self, text):\n",
    "        \"\"\"Split text into chunks of minimum length\"\"\"\n",
    "        if not text:\n",
    "            return []\n",
    "            \n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_length = 0\n",
    "        \n",
    "        for word in text.split():\n",
    "            current_chunk.append(word)\n",
    "            current_length += len(word) + 1\n",
    "            \n",
    "            if current_length >= self.min_text_length:\n",
    "                chunks.append(' '.join(current_chunk))\n",
    "                current_chunk = []\n",
    "                current_length = 0\n",
    "        \n",
    "        if current_chunk and current_length >= self.min_text_length / 2:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            \n",
    "        return chunks\n",
    "    \n",
    "    def collect_dataset(self, max_samples_per_class=1000):\n",
    "        \"\"\"Collect and prepare the complete dataset\"\"\"\n",
    "        print(\"Collecting English texts...\")\n",
    "        english_texts = self.collect_english_texts()\n",
    "        \n",
    "        print(f\"\\nTotal English texts collected: {len(english_texts)}\")\n",
    "        if english_texts:\n",
    "            print(f\"Average length of English texts: {np.mean([len(text) for text in english_texts]):.0f} characters\")\n",
    "        \n",
    "        print(\"\\nCollecting non-English texts...\")\n",
    "        non_english_texts = self.collect_non_english_texts()\n",
    "        \n",
    "        print(f\"\\nTotal non-English texts collected: {len(non_english_texts)}\")\n",
    "        if non_english_texts:\n",
    "            print(f\"Average length of non-English texts: {np.mean([len(text) for text in non_english_texts]):.0f} characters\")\n",
    "        \n",
    "        # Balance the dataset\n",
    "        min_samples = min(len(english_texts), len(non_english_texts), max_samples_per_class)\n",
    "        english_texts = english_texts[:min_samples]\n",
    "        non_english_texts = non_english_texts[:min_samples]\n",
    "        \n",
    "        print(\"\\nFinal Dataset Statistics:\")\n",
    "        print(f\"Number of English samples: {len(english_texts)}\")\n",
    "        print(f\"Number of non-English samples: {len(non_english_texts)}\")\n",
    "        \n",
    "        # Create labels\n",
    "        texts = english_texts + non_english_texts\n",
    "        labels = ['english'] * len(english_texts) + ['non-english'] * len(non_english_texts)\n",
    "        \n",
    "        return texts, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Language Classification System\n",
    "\n",
    "Comprehensive system for language classification with multiple models and evaluation:\n",
    "\n",
    "## Components\n",
    "1. **Classifiers**\n",
    "   - Random Forest\n",
    "   - SVM\n",
    "   - Neural Network (MLP)\n",
    "\n",
    "2. **Pipeline Features**\n",
    "   - Text preparation\n",
    "   - Feature extraction\n",
    "   - Model evaluation\n",
    "   - Cross-validation\n",
    "\n",
    "## Main Functions\n",
    "- `train_and_evaluate()`: Trains models and selects best performer\n",
    "- `predict()`: Makes predictions with confidence scores\n",
    "- `main()`: Demonstrates system with diverse test cases\n",
    "\n",
    "## Testing Suite\n",
    "Includes various test cases:\n",
    "- Short texts\n",
    "- Technical language\n",
    "- Mixed language indicators\n",
    "- Special characters and numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageClassificationSystem:\n",
    "    \"\"\"Complete language classification system with improvements\"\"\"\n",
    "    \n",
    "    def __init__(self, min_text_length=100, max_samples_per_class=1000):\n",
    "        self.min_text_length = min_text_length\n",
    "        self.max_samples_per_class = max_samples_per_class\n",
    "        \n",
    "        # Initialize classifiers with better parameters\n",
    "        self.classifiers = {\n",
    "            'random_forest': RandomForestClassifier(\n",
    "                n_estimators=200,\n",
    "                max_depth=20,\n",
    "                min_samples_leaf=5,\n",
    "                class_weight='balanced',\n",
    "                random_state=42\n",
    "            ),\n",
    "            'svm': SVC(\n",
    "                kernel='rbf',\n",
    "                probability=True,\n",
    "                class_weight='balanced',\n",
    "                random_state=42\n",
    "            ),\n",
    "            'neural_net': MLPClassifier(\n",
    "                hidden_layer_sizes=(200, 100, 50),\n",
    "                max_iter=500,\n",
    "                early_stopping=True,\n",
    "                validation_fraction=0.2,\n",
    "                random_state=42\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        self.feature_extractor = EnhancedLanguageFeatureExtractor(\n",
    "            max_features=2000\n",
    "        )\n",
    "        self.pipelines = {}\n",
    "        self.best_pipeline = None\n",
    "        \n",
    "    def prepare_text(self, text):\n",
    "        \"\"\"Prepare text for prediction by padding if necessary\"\"\"\n",
    "        if len(text) < self.min_text_length:\n",
    "            repetitions = (self.min_text_length // len(text)) + 1\n",
    "            text = (text + \" \") * repetitions\n",
    "        return text[:self.min_text_length]\n",
    "    \n",
    "    def build_pipelines(self):\n",
    "        \"\"\"Build classification pipelines\"\"\"\n",
    "        for name, classifier in self.classifiers.items():\n",
    "            self.pipelines[name] = Pipeline([\n",
    "                ('features', self.feature_extractor),\n",
    "                ('classifier', classifier)\n",
    "            ])\n",
    "    \n",
    "    def train_and_evaluate(self):\n",
    "        \"\"\"Train and evaluate all models\"\"\"\n",
    "        # Prepare data\n",
    "        print(\"Preparing dataset...\")\n",
    "        collector = MultilingualTextCollector(min_text_length=self.min_text_length)\n",
    "        texts, labels = collector.collect_dataset(max_samples_per_class=self.max_samples_per_class)\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            texts, labels, test_size=0.2, stratify=labels, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Build and train pipelines\n",
    "        print(\"\\nTraining models...\")\n",
    "        self.build_pipelines()\n",
    "        \n",
    "        # Train and evaluate each model\n",
    "        results = {}\n",
    "        for name, pipeline in self.pipelines.items():\n",
    "            print(f\"\\nEvaluating {name}...\")\n",
    "            \n",
    "            # Cross-validation\n",
    "            cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5)\n",
    "            print(f\"Cross-validation scores: {cv_scores}\")\n",
    "            print(f\"Average CV score: {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})\")\n",
    "            \n",
    "            # Train on full training set\n",
    "            pipeline.fit(X_train, y_train)\n",
    "            \n",
    "            # Evaluate on test set\n",
    "            test_score = pipeline.score(X_test, y_test)\n",
    "            print(f\"Test score: {test_score:.4f}\")\n",
    "            \n",
    "            # Detailed classification report\n",
    "            y_pred = pipeline.predict(X_test)\n",
    "            print(\"\\nClassification Report:\")\n",
    "            print(classification_report(y_test, y_pred))\n",
    "            \n",
    "            # Save results\n",
    "            results[name] = {\n",
    "                'cv_mean': cv_scores.mean(),\n",
    "                'cv_std': cv_scores.std(),\n",
    "                'test_score': test_score,\n",
    "                'pipeline': pipeline\n",
    "            }\n",
    "        \n",
    "        # Select best model based on both CV and test performance\n",
    "        best_model = max(results.items(), \n",
    "                        key=lambda x: (x[1]['cv_mean'] + x[1]['test_score']) / 2)\n",
    "        self.best_pipeline = best_model[1]['pipeline']\n",
    "        print(f\"\\nBest model: {best_model[0]}\")\n",
    "        print(f\"CV Score: {best_model[1]['cv_mean']:.4f}\")\n",
    "        print(f\"Test Score: {best_model[1]['test_score']:.4f}\")\n",
    "        \n",
    "        return self.best_pipeline\n",
    "    \n",
    "    def predict(self, text, get_probabilities=False):\n",
    "        \"\"\"Make prediction with text preparation\"\"\"\n",
    "        if self.best_pipeline is None:\n",
    "            raise ValueError(\"Model not trained. Call train_and_evaluate first.\")\n",
    "        \n",
    "        # Prepare text\n",
    "        prepared_text = self.prepare_text(text)\n",
    "        \n",
    "        if get_probabilities:\n",
    "            probs = self.best_pipeline.predict_proba([prepared_text])[0]\n",
    "            # Add confidence adjustment for very short texts\n",
    "            if len(text) < self.min_text_length:\n",
    "                # Reduce confidence for short texts\n",
    "                probs = (probs + 1) / 3\n",
    "            return probs\n",
    "        return self.best_pipeline.predict([prepared_text])[0]\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function with diverse test cases\"\"\"\n",
    "    print(\"Initializing Language Classification System...\")\n",
    "    system = LanguageClassificationSystem(\n",
    "        min_text_length=100,\n",
    "        max_samples_per_class=2000\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        best_classifier = system.train_and_evaluate()\n",
    "        \n",
    "        # Test with various lengths and styles\n",
    "        test_texts = [\n",
    "            # Very short\n",
    "            \"This is English.\",\n",
    "            \n",
    "            # Short with numbers and punctuation\n",
    "            \"Testing 123! Is this working properly?\",\n",
    "            \n",
    "            # Medium with variety\n",
    "            \"\"\"This text includes numbers (123), punctuation marks (!?.,), \n",
    "            and some UPPERCASE words. It should test various features.\"\"\",\n",
    "            \n",
    "            # Non-English looking text\n",
    "            \"Lorem ipsum dolor sit amet, consectetur adipiscing elit.\",\n",
    "            \n",
    "            # Mixed language indicators\n",
    "            \"English with émbelishments and école words.\",\n",
    "            \n",
    "            # Technical English\n",
    "            \"The API endpoint returns JSON data with UTF-8 encoding.\",\n",
    "        ]\n",
    "        \n",
    "        print(\"\\nTesting with diverse examples:\")\n",
    "        for i, test_text in enumerate(test_texts, 1):\n",
    "            prediction = system.predict(test_text)\n",
    "            probabilities = system.predict(test_text, get_probabilities=True)\n",
    "            \n",
    "            print(f\"\\nTest Example {i}:\")\n",
    "            print(f\"Length: {len(test_text)} characters\")\n",
    "            print(\"Text:\", test_text)\n",
    "            print(f\"Classification: {prediction}\")\n",
    "            print(\"Confidence scores:\")\n",
    "            for label, prob in zip(['english', 'non-english'], probabilities):\n",
    "                print(f\"  {label}: {prob:.4f}\")\n",
    "                \n",
    "            # Add warning for very short texts\n",
    "            if len(test_text) < system.min_text_length:\n",
    "                print(\"Warning: Text is shorter than recommended length. Results may be less reliable.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during execution: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Persistence\n",
    "\n",
    "Optional code for model serialization:\n",
    "- Saves trained model using pickle\n",
    "- Provides loading functionality for future use\n",
    "- Currently commented out for selective implementation\n",
    "\n",
    "Note: Uncomment and use as needed for model deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# import system\n",
    "\n",
    "# # Save model\n",
    "# with open('language_classifier.pkl', 'wb') as f:\n",
    "#     pickle.dump(system.best_pipeline, f)\n",
    "\n",
    "# # Load model\n",
    "# with open('language_classifier.pkl', 'rb') as f:\n",
    "#     loaded_model = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
